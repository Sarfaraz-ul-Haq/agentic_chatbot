{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "eyQKeEtNe8S-"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langgraph langchain-google-genai langchain-community langchain-core tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"learn_agentic_ai\""
      ],
      "metadata": {
        "id": "jvZG8D6ffACl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, List\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from pydantic import BaseModel\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "from langchain.output_parsers.structured import ResponseSchema\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from IPython.display import Image, display\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.types import Command\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(api_key=GEMINI_API_KEY, model=\"gemini-1.5-flash\", temperature=0)\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[List[AnyMessage], add_messages]\n",
        "    category: str\n",
        "    sentiment: str\n",
        "    response: str\n",
        "\n",
        "response_schemas = [\n",
        "    ResponseSchema(\n",
        "        name=\"category\",\n",
        "        description=\"The category of the query, one of: Order Issues, Delivery Status, Product Information, Feedback / Complaints, Handle General\"\n",
        "    )\n",
        "]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "\n",
        "def categorize(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Categorize the customer query into one of the following categories:\n",
        "    - Order Issues\n",
        "    - Delivery Status\n",
        "    - Product Information\n",
        "    - Feedback / Complaints\n",
        "    - Handle General\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"categorize node\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Categorize the following customer query into one of these categories: \"\n",
        "        \"Order Issues, Delivery Status, Product Information, Feedback / Complaints   Query: {query} {{format_instructions}}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    category = chain.invoke({\"query\": state[\"messages\"]}).content\n",
        "    return {\"category\": category}\n",
        "\n",
        "def analyze_sentiment(state: State) -> State:\n",
        "    \"\"\"Analyze the sentiment of the customer query - Sentiment options: Positive, Neutral, or Negative\"\"\"\n",
        "    print(\"analyze_sentiment node\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Analyze the sentiment of the following customer query. \"\n",
        "        \"Respond with either 'Positive', 'Neutral', or 'Negative'. Query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    sentiment = chain.invoke({\"query\": state[\"messages\"]}).content\n",
        "    return {\"sentiment\": sentiment}\n",
        "\n",
        "def order_issues(state: State) -> State:\n",
        "    \"\"\"Handle customer queries categorized as 'Order Issues'.\"\"\"\n",
        "    print(\"order_issues node\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a response to the following order issue query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"messages\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def delivery_status(state: State) -> State:\n",
        "    \"\"\"Handle customer queries categorized as 'Delivery Status'.\"\"\"\n",
        "    print(\"delivery_status node\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a response to the following delivery status query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"messages\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def feedback_complaints(state: State) -> State:\n",
        "    \"\"\"Handle customer queries categorized as 'Feedback / Complaints'.\"\"\"\n",
        "    print(\"feedback_complaints node\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a response to the following feedback or complaints query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"messages\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def handle_general(state: State) -> State:\n",
        "    \"\"\"Handle customer queries categorized as 'General'.\"\"\"\n",
        "    print(\"handle_general node\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a general support response to the following query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"messages\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def escalate(state: State) -> State:\n",
        "    \"\"\"Escalate the query to a human agent due to negative sentiment.\"\"\"\n",
        "    print(\"escalate node\")\n",
        "    return {\"response\": \"This query has been escalated to a human agent due to its negative sentiment.\"}\n",
        "\n",
        "def route_query(state: State) -> str:\n",
        "    \"\"\"\n",
        "    Route the query based on its sentiment and category.\n",
        "\n",
        "    Args:\n",
        "        state (State): The current state containing user messages, category, and sentiment.\n",
        "\n",
        "    Returns:\n",
        "        str: The next node name based on the routing logic.\n",
        "    \"\"\"\n",
        "    if state[\"sentiment\"] == \"Negative\":\n",
        "        return \"escalate\"\n",
        "    elif state[\"category\"] == \"Order Issues\":\n",
        "        return \"order_issues\"\n",
        "    elif state[\"category\"] == \"Delivery Status\":\n",
        "        return \"delivery_status\"\n",
        "    elif state[\"category\"] == \"Feedback / Complaints\":\n",
        "        return \"feedback_complaints\"\n",
        "    else:\n",
        "        return \"handle_general\"\n",
        "\n",
        "# Define the LangGraph workflow\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Add nodes to the graph\n",
        "graph_builder.add_node(\"categorize\", categorize)\n",
        "graph_builder.add_node(\"analyze_sentiment\", analyze_sentiment)\n",
        "graph_builder.add_node(\"order_issues\", order_issues)\n",
        "graph_builder.add_node(\"delivery_status\", delivery_status)\n",
        "graph_builder.add_node(\"feedback_complaints\", feedback_complaints)\n",
        "graph_builder.add_node(\"handle_general\", handle_general)\n",
        "graph_builder.add_node(\"escalate\", escalate)\n",
        "\n",
        "# Add edges to the graph\n",
        "graph_builder.add_edge(START, \"categorize\")\n",
        "graph_builder.add_edge(\"categorize\", \"analyze_sentiment\")\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"analyze_sentiment\",\n",
        "    route_query,\n",
        "    {\n",
        "        \"escalate\": \"escalate\",\n",
        "        \"order_issues\": \"order_issues\",\n",
        "        \"delivery_status\": \"delivery_status\",\n",
        "        \"feedback_complaints\": \"feedback_complaints\",\n",
        "        \"handle_general\": \"handle_general\",\n",
        "    }\n",
        ")\n",
        "graph_builder.add_edge(\"order_issues\", END)\n",
        "graph_builder.add_edge(\"delivery_status\", END)\n",
        "graph_builder.add_edge(\"feedback_complaints\", END)\n",
        "graph_builder.add_edge(\"handle_general\", END)\n",
        "graph_builder.add_edge(\"escalate\", END)\n",
        "\n",
        "# Compile and display the graph\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "\n",
        "def test_langgraph_agent(query: str):\n",
        "    \"\"\"\n",
        "    Test the LangGraph agent with a sample query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The customer query to test the workflow.\n",
        "\n",
        "    Returns:\n",
        "        dict: Final state including category, sentiment, and response.\n",
        "    \"\"\"\n",
        "    # Initialize the starting state\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"category\": \"\",\n",
        "        \"sentiment\": \"\",\n",
        "        \"response\": \"\"\n",
        "    }\n",
        "\n",
        "    config = { \"configurable\": { \"thread_id\": 1 }}\n",
        "        # Run the query through the compiled LangGraph\n",
        "\n",
        "    output = graph.invoke(initial_state, config)\n",
        "\n",
        "    # Display the results\n",
        "    print(\"Query:\", query)\n",
        "    print(\"Category:\", output[\"category\"])\n",
        "    print(\"Sentiment:\", output[\"sentiment\"])\n",
        "    print(\"Response:\", output[\"response\"])\n",
        "\n",
        "# Example queries to test\n",
        "test_queries = [\n",
        "    \"I need help with a refund for my order.\",\n",
        "    \"When will my package arrive?\",\n",
        "    \"I have a complaint about the quality of the product.\",\n",
        "    \"What are your business hours?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(\"\\n--- Testing Query ---\")\n",
        "    test_langgraph_agent(query)\n"
      ],
      "metadata": {
        "id": "Xll5_OohfCU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}